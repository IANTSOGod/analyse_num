{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUEJU7CL2zWm"
   },
   "source": [
    "# **VECTORISATION DES PHRASES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43483,
     "status": "ok",
     "timestamp": 1758792322517,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "FtLblbb2-VCj",
    "outputId": "5e60540a-0de9-463a-c28e-1e049b91b733"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import Counter\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1758792322653,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "xqNTer9V2-es"
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence, n=4):\n",
    "  words = sentence.strip()\n",
    "  return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1758792322657,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "prxAT-GO6hUK"
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(corpus, K=5000, n=4):\n",
    "\n",
    "    counter = Counter()\n",
    "\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenize(sentence, n)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    most_common = counter.most_common(K)\n",
    "    vocab = {token: idx for idx, (token, _) in enumerate(most_common)}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758792322667,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "kmpBl2Zx-Kke"
   },
   "outputs": [],
   "source": [
    "def compute_tf(tokens, vocab):\n",
    "    tf = np.zeros(len(vocab))\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            tf[vocab[token]] += 1\n",
    "    if tf.sum() > 0:\n",
    "        tf = tf / tf.sum()\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758792322678,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "OL6NRBfrS0NM"
   },
   "outputs": [],
   "source": [
    "def compute_tdf(corpus, vocab, n=4):\n",
    "    N = len(corpus)\n",
    "    idf = np.zeros(len(vocab))\n",
    "\n",
    "    for sentence in corpus:\n",
    "        tokens = set(tokenize(sentence, n))\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                idf[vocab[token]] += 1\n",
    "\n",
    "    idf = np.log((N + 1) / (idf + 1)) + 1\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1758792322684,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "X5u2ZcjzXSmf"
   },
   "outputs": [],
   "source": [
    "def vectorize(corpus, K=5000, n=4, vocab=None, idf=None):\n",
    "    X = []\n",
    "    tu = []\n",
    "    if vocab == None:\n",
    "        vocab = build_vocabulary(corpus, K=K, n=n)\n",
    "        idf = compute_tdf(corpus, vocab, n)\n",
    "        tu.append(vocab)\n",
    "        tu.append(idf)\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenize(sentence, n=n)\n",
    "        tf = compute_tf(tokens, vocab)\n",
    "        X.append(tf * idf)\n",
    "\n",
    "    tu.append(X)    \n",
    "    return tuple(tu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1758792322825,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "APzvTJOTqaAd"
   },
   "outputs": [],
   "source": [
    "def vectorize_y(Y):\n",
    "    Y_unique = set(Y)\n",
    "    Y_set = {}\n",
    "    for index, val in enumerate(Y_unique):\n",
    "        Y_set[val] = index\n",
    "    Y_final = []\n",
    "    for y in Y:\n",
    "        Y_final.append(Y_set[y])\n",
    "    return np.array(Y_final), Y_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1758792322830,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "dYvOnNUcZVn2"
   },
   "outputs": [],
   "source": [
    "def save_X( X_vect, file=\"/content/drive/MyDrive/new_dataX.npy\"):\n",
    "    np.save(file, X_vect)\n",
    "    print(\"data X sauvegardée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1758792322836,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "X4Sdy_haac-O"
   },
   "outputs": [],
   "source": [
    "def save_vocab(vocab, file=\"/content/drive/MyDrive/vocab.json\"):\n",
    "    with open(file, \"w\") as f:\n",
    "        json.dump(vocab, f)\n",
    "    print(\"Vocabulaire X sauvegardée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1758792323038,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "n8xCaYWzauC2"
   },
   "outputs": [],
   "source": [
    "def save_idf(idf_vect, file=\"/content/drive/MyDrive/idfVect.npy\"):\n",
    "    np.save(file, idf_vect)\n",
    "    print(\"idf vecteur Y sauvegardée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1758792323063,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "BKifqj5QrXxg"
   },
   "outputs": [],
   "source": [
    "def save_Y(Y, file=\"/content/drive/MyDrive/dataY.npy\"):\n",
    "    np.save(file, Y)\n",
    "    print(\"data Y sauvegardée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758792323065,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "0xjDu1eatFLz"
   },
   "outputs": [],
   "source": [
    "def save_vocabY(Y, file=\"/content/drive/MyDrive/vocabY.json\"):\n",
    "    with open(file, \"w\") as f:\n",
    "        json.dump(Y, f)\n",
    "    print(\"Vocabulaire Y sauvegardée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_one(text):\n",
    "    sentence = [text]\n",
    "\n",
    "    idf = np.load(\"idfFit.npy\")\n",
    "    json_path = \"vocabFit.json\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        vocabX = json.load(f)\n",
    "        \n",
    "    \n",
    "    sentVect = vectorize(sentence, K=5000, n=4, vocab=vocabX, idf=idf )\n",
    "    sentVect = sentVect[0]\n",
    "    \n",
    "    svd = joblib.load(\"svdmodel2.plk\")\n",
    "    \n",
    "    sent = svd.transform(sentVect)\n",
    "    print(sent.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return sent"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
