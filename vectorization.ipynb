{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUEJU7CL2zWm"
   },
   "source": [
    "# **VECTORISATION DES PHRASES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43483,
     "status": "ok",
     "timestamp": 1758792322517,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "FtLblbb2-VCj",
    "outputId": "5e60540a-0de9-463a-c28e-1e049b91b733"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import Counter\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1758792322653,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "xqNTer9V2-es"
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence, n=4):\n",
    "  words = sentence.strip()\n",
    "  return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1758792322657,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "prxAT-GO6hUK"
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(corpus, K=5000, n=4):\n",
    "\n",
    "    counter = Counter()\n",
    "\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenize(sentence, n)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    most_common = counter.most_common(K)\n",
    "    vocab = {token: idx for idx, (token, _) in enumerate(most_common)}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758792322667,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "kmpBl2Zx-Kke"
   },
   "outputs": [],
   "source": [
    "def compute_tf(tokens, vocab):\n",
    "    tf = np.zeros(len(vocab))\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            tf[vocab[token]] += 1\n",
    "    if tf.sum() > 0:\n",
    "        tf = tf / tf.sum()\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758792322678,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "OL6NRBfrS0NM"
   },
   "outputs": [],
   "source": [
    "def compute_tdf(corpus, vocab, n=4):\n",
    "    N = len(corpus)\n",
    "    idf = np.zeros(len(vocab))\n",
    "\n",
    "    for sentence in corpus:\n",
    "        tokens = set(tokenize(sentence, n))\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                idf[vocab[token]] += 1\n",
    "\n",
    "    idf = np.log((N + 1) / (idf + 1)) + 1\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1758792322684,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "X5u2ZcjzXSmf"
   },
   "outputs": [],
   "source": [
    "def vectorize(corpus, K=5000, n=4, vocab=None, idf=None):\n",
    "    X = []\n",
    "    tu = []\n",
    "    if vocab == None:\n",
    "        vocab = build_vocabulary(corpus, K=K, n=n)\n",
    "        idf = compute_tdf(corpus, vocab, n)\n",
    "        tu.append(vocab)\n",
    "        tu.append(idf)\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenize(sentence, n=n)\n",
    "        tf = compute_tf(tokens, vocab)\n",
    "        X.append(tf * idf)\n",
    "\n",
    "    tu.append(X)    \n",
    "    return tuple(tu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1758792322825,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "APzvTJOTqaAd"
   },
   "outputs": [],
   "source": [
    "def vectorize_y(Y):\n",
    "    Y_unique = set(Y)\n",
    "    Y_set = {}\n",
    "    for index, val in enumerate(Y_unique):\n",
    "        Y_set[val] = index\n",
    "    Y_final = []\n",
    "    for y in Y:\n",
    "        Y_final.append(Y_set[y])\n",
    "    return np.array(Y_final), Y_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1758792322830,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "dYvOnNUcZVn2"
   },
   "outputs": [],
   "source": [
    "def save_X( X_vect, file=\"/content/drive/MyDrive/new_dataX.npy\"):\n",
    "    np.save(file, X_vect)\n",
    "    print(\"data X sauvegardée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1758792322836,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "X4Sdy_haac-O"
   },
   "outputs": [],
   "source": [
    "def save_vocab(vocab, file=\"/content/drive/MyDrive/vocab.json\"):\n",
    "    with open(file, \"w\") as f:\n",
    "        json.dump(vocab, f)\n",
    "    print(\"Vocabulaire X sauvegardée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1758792323038,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "n8xCaYWzauC2"
   },
   "outputs": [],
   "source": [
    "def save_idf(idf_vect, file=\"/content/drive/MyDrive/idfVect.npy\"):\n",
    "    np.save(file, idf_vect)\n",
    "    print(\"idf vecteur Y sauvegardée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1758792323063,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "BKifqj5QrXxg"
   },
   "outputs": [],
   "source": [
    "def save_Y(Y, file=\"/content/drive/MyDrive/dataY.npy\"):\n",
    "    np.save(file, Y)\n",
    "    print(\"data Y sauvegardée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758792323065,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "0xjDu1eatFLz"
   },
   "outputs": [],
   "source": [
    "def save_vocabY(Y, file=\"/content/drive/MyDrive/vocabY.json\"):\n",
    "    with open(file, \"w\") as f:\n",
    "        json.dump(Y, f)\n",
    "    print(\"Vocabulaire Y sauvegardée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71914,
     "status": "ok",
     "timestamp": 1758792688381,
     "user": {
      "displayName": "Tsito Rabemananjara",
      "userId": "14764735742114144836"
     },
     "user_tz": -180
    },
    "id": "ElrukK4bttc_",
    "outputId": "069cb3ac-99cd-43b4-d925-6375b8cb48a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice réduite (shape) : (22000, 600)\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(\"dataset.csv\")\n",
    "corpus = df['Text'].tolist()\n",
    "vocab, idf, X = vectorize(corpus, K=5000, n=4)\n",
    "svd = TruncatedSVD(n_components=600, random_state=42)\n",
    "X_reduit = svd.fit_transform(X)\n",
    "\n",
    "\n",
    "# Étape 2 : Appliquer PCA\n",
    "# X_reduit = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Matrice réduite (shape) :\", X_reduit.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminer\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(svd, \"svdmodel2.plk\")\n",
    "print(\"Terminer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data X sauvegardée\n",
      "Vocabulaire X sauvegardée\n",
      "idf vecteur Y sauvegardée\n"
     ]
    }
   ],
   "source": [
    "save_X(X_reduit, file=\"Xfit.npy\")\n",
    "save_vocab(vocab, file=\"vocabFit.json\")\n",
    "save_idf(idf, file=\"idfFit.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Swedish': 0, 'Pushto': 1, 'Latin': 2, 'Korean': 3, 'Turkish': 4, 'Urdu': 5, 'Arabic': 6, 'Thai': 7, 'Dutch': 8, 'French': 9, 'English': 10, 'Persian': 11, 'Spanish': 12, 'Russian': 13, 'Portugese': 14, 'Estonian': 15, 'Chinese': 16, 'Indonesian': 17, 'Japanese': 18, 'Tamil': 19, 'Romanian': 20, 'Hindi': 21}\n",
      "data Y sauvegardée\n",
      "Vocabulaire Y sauvegardée\n"
     ]
    }
   ],
   "source": [
    "corpusY = df['language'].tolist()\n",
    "vectY, vocabY = vectorize_y(corpusY)\n",
    "save_Y(vectY, file=\"Yfitsq.npy\")\n",
    "save_vocabY(vocabY, file=\"vocabYFitsq.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "(1, 600)\n",
      "data X sauvegardée\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"Bonjour la famille j espere que vous allez bien, je vous ai envoye le dernier paiement , il est co\"]\n",
    "print(len(sentence[0]))\n",
    "\n",
    "idf = np.load(\"idfFit.npy\")\n",
    "json_path = \"vocabFit.json\"\n",
    "with open(json_path, 'r') as f:\n",
    "    vocabX = json.load(f)\n",
    "    \n",
    "\n",
    "sentVect = vectorize(sentence, K=5000, n=4, vocab=vocabX, idf=idf )\n",
    "sentVect = sentVect[0]\n",
    "\n",
    "svd = joblib.load(\"svdmodel2.plk\")\n",
    "\n",
    "sent = svd.transform(sentVect)\n",
    "print(sent.shape)\n",
    "\n",
    "\n",
    "save_X(sent, file=\"test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
